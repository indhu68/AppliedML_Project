{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMXrj+daSrbX1A9XYT6GmOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indhu68/AppliedML_Project/blob/main/AppliedMLProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIJh8ynFxj_G",
        "outputId": "4b39619b-391c-47d8-d324-f7d911e1d570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "metadata": {
        "id": "YlfkiM5Hx6m8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.version)\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "print(np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS9W124Xx7FJ",
        "outputId": "d78296af-3660-4da1-f906-3fc95403cbb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'numpy.version' from '/usr/local/lib/python3.10/dist-packages/numpy/version.py'>\n",
            "Using tensorflow version: 2.15.0\n",
            "1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200000\n",
        "########## BIT FLIPPING OFF\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "\n",
        "# tr is a random uniform distribution of numbers between 1 and M, with the length of the vector being the batch size\n",
        "\n",
        "\n",
        "tr = np.floor(np.random.uniform(1,M, batch_size))\n",
        "#print(tr)\n",
        "print(np.size(tr))\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(1,M)\n",
        "rp2 = np.flip(np.arange(1,M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "#tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#print(tr)\n",
        "\n",
        "s_ind = {}\n",
        "for j in range(1,M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "#print(s_ind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NoOevrVx9CR",
        "outputId": "219eb9cd-a185-40df-e446-00b19eba0038"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "metadata": {
        "id": "drLHC5yoyCjA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "\n",
        "BW = 10*10**6; #in Hz\n",
        "PLE = 2.8; #Path Loss exponent\n",
        "Beta = {}\n",
        "Pr = {}\n",
        "SNR_var = {}\n",
        "ad_noise_SNR = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "NoW = 0.02\n",
        "\n",
        "Transmitted_Power = 20 #in dBm\n",
        "Noise_val = 95 #in -dBm\n",
        "\n",
        "noise_factor = Transmitted_Power + Noise_val\n",
        "#|β| =λ/(4πD)2.31\n",
        "\n",
        "\n",
        "s = np.floor(np.random.uniform(1,M, batch_size))\n",
        "s = s.astype('int64')\n",
        "#s = ([replacer(n, n) for n in s])\n",
        "std_shadow = 3\n",
        "rand1 = np.random.normal(0,std_shadow,batch_size)\n",
        "for i in range(batch_size):\n",
        "  Beta[i] = 10*(np.log10(lamda/((4*np.pi*s[i])**PLE)))\n",
        "  SNR_var[i] = Beta[i] + noise_factor + rand1[i]\n",
        "#SNR_var = SNR_var + np.random.normal(0, 3, batch_size)\n",
        "  #SNR_var[i] = 10*(np.log10(np.divide((Pt*[(Beta[i])**2]),NoW)))\n",
        "  #SNR_var[i] = ((M)/((s[i]+1)*4))*25   #For M=8\n",
        "  #SNR_var[i] = ((M)/((s[i]+1)*4))      #For M=256\n",
        "\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_SNR.append(i)\n",
        "print(np.shape(ad_noise_SNR))\n",
        "ad_noise_SNR = np.transpose(np.tile(ad_noise_SNR, (2,2,1)))\n",
        "print(np.shape(ad_noise_SNR))\n",
        "\n",
        "#print(np.shape(np.tile(ad_noise_SNR[7], (1000, 1, 1))))\n",
        "#print(np.shape(tr[7]*np.ones(batch_size)))\n",
        "rand2 = np.random.normal(0,std_shadow,batch_size)\n",
        "\n",
        "for j in range(1,M):\n",
        "\n",
        "\n",
        "  #SNR_var[j] = ((M)/((j+1)*4))*25\n",
        "  #SNR_var[j] = ((M)/((j+1)*4))\n",
        "  Beta[j] = 10*(np.log10(lamda/((4*np.pi*j)**2.8)))\n",
        "  SNR_var[j] = Beta[j] + noise_factor + rand2[j]\n",
        "\n",
        "const_noise_SNR = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for j in SNR_var.values():\n",
        "  const_noise_SNR.append(j)\n",
        "const_noise_SNR = np.transpose(np.tile(const_noise_SNR, (2,2,1)))\n",
        "#print(const_noise_std)\n",
        "\n",
        "#Choosing 15dB SNR as the constant SNR for the TB Approach\n",
        "seven_noise_SNR = np.add(np.ones(batch_size)*7, np.random.normal(0,3,batch_size))\n",
        "\n",
        "seven_noise_SNR = np.transpose(np.tile(seven_noise_SNR, (2,2,1)))\n",
        "#print(np.random.normal(0,std_shadow,batch_size))\n",
        "\n",
        "tr_noise_SNR = []\n",
        "rand3 = np.random.normal(0,std_shadow,batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "\n",
        "  #SNR_var[i] = ((M)/((tr[i]+1)*4))*25\n",
        "  #SNR_var[i] = ((M)/((tr[i]+1)*4))\n",
        "\n",
        "  Beta[i] = 10*(np.log10(lamda/((4*np.pi*tr[i])**2.8)))\n",
        "  SNR_var[i] = Beta[i] + noise_factor + rand3[i]\n",
        "for i in SNR_var.values():\n",
        "  tr_noise_SNR.append(i)\n",
        "\n",
        "tr_noise_SNR = np.transpose(np.tile(tr_noise_SNR, (2,2,1)))\n",
        "#print(s)\n",
        "\n",
        "#print(ad_noise_SNR[7])\n",
        "#print(np.tile(ad_noise_SNR[7], (1000, 1, 1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX2HeFnVyIQw",
        "outputId": "5eaafaae-1f33-4793-a59f-5658e7478472"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000,)\n",
            "(200000, 2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())\n",
        "        self.graph = None\n",
        "        self.sess = None\n",
        "        self.vars = None\n",
        "        self.saver = None\n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:\n",
        "            self.load(filename)\n",
        "        return\n",
        "\n",
        "\n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "\n",
        "            # Transmitter\n",
        "            s = (tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64))\n",
        "\n",
        "            ad_noise_std = tf.random_uniform(shape=[batch_size], minval=0, maxval=M, dtype=tf.int64)\n",
        "            ones = tf.ones([2,2,1],dtype=tf.int64)\n",
        "            ad_noise_std = tf.transpose(ad_noise_std*ones)\n",
        "            #ad_noise_std = tf.transpose(tf.tile(ad_noise_std, [2,2,1]))\n",
        "            x = self.encoder(s)\n",
        "\n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[200000,2,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std)\n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1\n",
        "\n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "\n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(0))\n",
        "\n",
        "            #Defining the i_matrix\n",
        "            limit = 256\n",
        "            i = tf.range(limit)\n",
        "            multiply = tf.constant([200000])\n",
        "            i_matrix = tf.reshape(tf.tile(i, multiply), [ multiply[0], tf.shape(i)[0]])\n",
        "            print(i_matrix)\n",
        "            #e_si = (tf.cast(s[:, tf.newaxis], tf.float32)) - (tf.cast(i_matrix,tf.float32))\n",
        "            #e_si = tf.math.square(tf.math.divide((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)), tf.transpose(tf.cast(i_matrix,tf.float32))))\n",
        "            e_si = tf.math.square((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)))\n",
        "            e_si = tf.transpose(e_si)\n",
        "            b = tf.nn.softmax(s_hat)\n",
        "\n",
        "            cost_func = tf.math.multiply(b,e_si)\n",
        "\n",
        "            sum_cf = tf.math.multiply( tf.math.sqrt(tf.reduce_sum(cost_func, axis = 1) + 0.000001 ), (tf.cast(1/(s+ 1), tf.float32)))\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step\n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy + sum_cf)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later\n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                #'s': (np.floor(np.random.uniform(0,256, 1000))).astype('int64'),\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }\n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "\n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "\n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "        return x\n",
        "\n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "\n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "\n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''\n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''\n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "\n",
        "    def plot_constellation(self, noise_std, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M), noise_std)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):\n",
        "              if (i <=150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))\n",
        "              if (i > 150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"red\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))\n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')\n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "\n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)\n",
        "\n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "\n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0))\n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))\n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "    def end2end_bler(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['bler'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0))\n",
        "\n",
        "\n",
        "    def train(self, training_params, validation_params):\n",
        "\n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):\n",
        "            s,batch_size, lr, ebnodb, iterations = params\n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "\n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "\n",
        "            for i in range(iterations):\n",
        "                self.train_step(s,batch_size, ebnodb, lr)\n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)\n",
        "        return\n",
        "\n",
        "    def train_step(self,s, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict={self.vars['s']: s, self.vars['batch_size']: batch_size,  self.vars['noise_std']: self.EbNo2Sigma(ebnodb), self.vars['lr']: lr})\n",
        "        return\n",
        "\n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out))\n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "\n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.\n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "\n",
        "\n",
        "\n",
        "    def bler_sim1(self,s,  ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.\n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "          #for ebnodb in ebnodbs:\n",
        "            #print(s, batch_size, ebnodb)\n",
        "          bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_e2e_feed_dict(batch_size, np.transpose(np.tile(np.ones(1000)*ebnodb, (2,2,1))), s, lr=0)) for ebnodb in ebnodbs])\n",
        "\n",
        "          BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "\n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "metadata": {
        "id": "qHNIeZ3JyPBv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZ3lvXCvyUgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}